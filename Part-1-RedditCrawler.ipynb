{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0aeaac00cbc5411daab1ac54ca6342c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d4e56c47a8694a668d32acf470b60c63",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ca2411485ab7438cb1caed40c611fdbf",
              "IPY_MODEL_6431ff0f780b4f96833bf0706bfa82d1"
            ]
          }
        },
        "d4e56c47a8694a668d32acf470b60c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca2411485ab7438cb1caed40c611fdbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_86ec3a0e3a2241d6b54552abb4591c4f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5000b432518545768c9a5cb3e6e1586e"
          }
        },
        "6431ff0f780b4f96833bf0706bfa82d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47b370f03d59440b976db267ef608382",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:13&lt;00:00,  4.54s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_148d90aa0cf34511a8e1bc373ba77c93"
          }
        },
        "86ec3a0e3a2241d6b54552abb4591c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5000b432518545768c9a5cb3e6e1586e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47b370f03d59440b976db267ef608382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "148d90aa0cf34511a8e1bc373ba77c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEYYYxhkYCX6",
        "colab_type": "text"
      },
      "source": [
        "## **REDDIT DATA CRAWLER**\n",
        "* This data crawler uses [pushlift ](https://github.com/pushshift/api) to scrape JSONs from Reddit's r\\india and dump the data to a csv.\n",
        "* Checkpoints are enabled for this implementation you can save and load checkpoints in form of pickle files (Implementation Below)\n",
        "* All warnings exception and http request results are handled and kept in a logger file.\n",
        "* By providing 4 parameters you can you can extract any number of posts in any period of time\n",
        "* The script for this is also available, which enables you to create a csv of data in only one line of code\n",
        "* To avoid hot topics to form a bias the posts have been taken over a wide period - 2 years of data has been scraped",
        "* A total of ***365,000*** submissions have been scraped from the subreddit where only ***48,380*** submissions are still available on Reddit and hence are a part of the dataset.\n",
        "* A committed version of this notebook is available on [kaggle](https://www.kaggle.com/someshsingh22/redditcrawlertest?scriptVersionId=31577144)",
        "* To scrape 365K submissions over 2 years this takes a total of ***~70 minutes*** where the runtime was only ***~20 minutes*** and wait-time was ***~50 minutes***s, which can be easily made 8x faster by changing the sleep parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsTrojDVOqdL",
        "colab_type": "text"
      },
      "source": [
        "### **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ewGcyMc0gPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0dc4286f-0f1d-4ae7-94a6-c9da4fa86072"
      },
      "source": [
        "import requests #For querying pushlift to extract json files from reddit\n",
        "import time #For generating timestamps\n",
        "import json #To handle json files from pushlift\n",
        "import logging  #Logging files for error handling\n",
        "import datetime #To convert unix timestamps to dates\n",
        "import pickle #to load and save from checkpoints\n",
        "from tqdm import tqdm_notebook #tqdm range to gauge progress\n",
        "import matplotlib.pyplot as plt #for analytics\n",
        "import seaborn as sns\n",
        "import pandas as pd #convert extracted data to csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf57sw16TR9g",
        "colab_type": "text"
      },
      "source": [
        "### **Create Logger for maintaining log file for errors and progress**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ummtn20TRS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create and configure logger \n",
        "logging.basicConfig(filename=\"crawler.log\", \n",
        "                    format='%(asctime)s %(message)s', \n",
        "                    filemode='w',\n",
        "                    level=logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LS_SZSPQEhT",
        "colab_type": "text"
      },
      "source": [
        "### **Reddit Crawler Class**\n",
        "The class takes four parameters for extraction of data\n",
        "\n",
        "\n",
        "1.   size : Number of results to lookup from (Advised $ 100 \\leq size \\leq 500 $ to avoid DNS blocking).\n",
        "2.   start : UNIX timestamp of the date from where the scraping should begin.\n",
        "3.   difference : The leap of time between two queries, the time-range (in days) to query reddit.\n",
        "4.   sleep : The delay between successive queris to pushlift (Advised to keep $ \\geq 1 $).\n",
        "\n",
        "Invalid parameters are handled as exceptions and all runtime errors along with the associated URLs are logged in crawler.log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f42VsErkCsri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Crawler:\n",
        "  def __init__(self, size=250, start=time.time(), difference=7, sleep=1):\n",
        "    \n",
        "    #Data Collected by the crawler\n",
        "    self.data={\n",
        "        'Author' : [],\n",
        "        'Title'  : [],\n",
        "        'Flair'  : [],\n",
        "        'Text'   : [],\n",
        "    }\n",
        "\n",
        "    self.stats=[] #Number of posts collected on every leap of time\n",
        "    self.sleep, self.size, self.start, self.difference = sleep, size, start, difference #Init the parameters\n",
        "    self.__validate__() #Validate Parmaeters and log warnings\n",
        "    self.difference=self.difference*24*3600 #set difference to day format\n",
        "    self.current=self.start #set timer for query to time of init\n",
        "    self.url_generator=self._url_generator() #create a url generator\n",
        "    self.url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=india&size={}&{}' #url format for pushlift\n",
        "\n",
        "  #Validator for Web Crawler\n",
        "  def __validate__(self):\n",
        "    #validate sleep value to be a number >= 1\n",
        "    try :\n",
        "      assert self.sleep >= 1 and (isinstance(self.sleep,int) or isinstance(self.sleep,float))\n",
        "    except:\n",
        "      logging.warning(\"Invalid sleep value, may cause DNS server blocking, set to 1\")\n",
        "      self.sleep=1\n",
        "\n",
        "    #validate query size to be a number <= 500\n",
        "    try :\n",
        "      assert self.size <= 500 and self.size>= 100 and isinstance(self.size,int)\n",
        "    except:\n",
        "      logging.warning(\"Invalid query size, may cause DNS server blocking, set to 500\")\n",
        "      self.size=500\n",
        "\n",
        "    #Validate start to be valid a present\\past timestamp\n",
        "    try :\n",
        "      self.start=int(self.start)\n",
        "      assert self.start <= time.time() and isinstance(self.start,int)\n",
        "    except:\n",
        "      logging.warning(\"Invalid start time, being set to current\")\n",
        "      self.start=int(time.time())\n",
        "    \n",
        "    #Validate the difference to be a valid positive number\n",
        "    try :\n",
        "      assert isinstance(self.difference,int) and self.difference > 0\n",
        "    except:\n",
        "      logging.warning(\"Invalid difference, setting to a week\")\n",
        "      self.difference=7\n",
        "\n",
        "  #URL Generator for pushlift\n",
        "  def _url_generator(self):\n",
        "    while True:\n",
        "      timestamp='before={}&after={}'.format(self.current, self.current-self.difference) #Get timestamp\n",
        "      yield self.url.format(self.size,timestamp) #get URL\n",
        "      self.current-=self.difference #Update Current time\n",
        "\n",
        "  #Process json output from pushlift to data\n",
        "  def process_json(self,jsons):\n",
        "    for json in jsons:\n",
        "      self.data['Author'].append(json['author'] if 'author' in json else None)\n",
        "      self.data['Title'].append(json['title'] if 'title' in json else None)\n",
        "      self.data['Text'].append(json['selftext'] if 'selftext' in json else None)\n",
        "      self.data['Flair'].append(json['link_flair_text'] if 'link_flair_text' in json else None)\n",
        "\n",
        "  #Query and update the data from Web Crawler\n",
        "  def query(self):\n",
        "    loc_url=next(self.url_generator) #generate URL\n",
        "    try:\n",
        "      r = requests.get(loc_url) #get JSON\n",
        "      data = json.loads(r.text) #load JSON to dict format\n",
        "\n",
        "      #filter deleted/removed and posts without text\n",
        "      jsons=[post for post in data['data'] if 'selftext' in post and not (post['selftext'] ==\"[removed]\" or post['selftext'] ==\"[deleted]\" or post['selftext']==\"\")]\n",
        "      \n",
        "      #process jsons to dict\n",
        "      self.process_json(jsons=jsons)\n",
        "\n",
        "      #append the datestamp and number of valid posts fetched in this leap\n",
        "      self.stats.append((datetime.datetime.fromtimestamp(self.current).strftime('%Y-%m-%d'),len(jsons)))\n",
        "\n",
        "      #delay interval to prevent DNS blocking\n",
        "      time.sleep(self.sleep)\n",
        "      logging.info(\"Query Successfull at {}\".format(loc_url))\n",
        "    except:\n",
        "      logging.error(\"Query Failed at {}\".format(loc_url))\n",
        "\n",
        "  #to save progress for multi stop database generation and save the checkpoint to a JSON\n",
        "  def save(self,pre=\"\"):\n",
        "    logging.info(\"Pickle dumped to {}.pkl\".format(self.current))\n",
        "    with open(pre+('{}.pkl'.format(self.current)), 'wb+') as f:\n",
        "        pickle.dump([self.data, self.stats], f)\n",
        "\n",
        "  #to load from a previous checkpoint\n",
        "  def load(self,js):\n",
        "    with open(js,'rb') as f:\n",
        "      self.data, self.stats = pickle.load(f)\n",
        "    self.current = int(js.split('/')[-1][:-4]) #init current stamp to saved stamp\n",
        "    self.timer=self._url_generator() #init generator from loaded stamp\n",
        "\n",
        "  #dump data to csv and stats to pkl\n",
        "  def dump(self,pre=\"\"):\n",
        "    self.csv=pd.DataFrame() #csv init\n",
        "    for key,values in self.data.items(): #create csv\n",
        "      self.csv[key]=values\n",
        "    self.csv.to_csv(pre+'raw_data.csv',index=False) #dump .csv\n",
        "    with open(pre+'stats.pkl', 'wb') as f:\n",
        "        pickle.dump(self.stats, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwV1Emr6pFIX",
        "colab_type": "text"
      },
      "source": [
        "### **Functionality and use case :**\n",
        "* Initialize the Crawler class and set the four parameters of choice (optional)\n",
        "* To start scraping loop `Object.query()` for the number of defined leaps\n",
        "* Load checkpoints by : `Object.load(path_to_pkl_file.pkl)`\n",
        "* To continue looping follow the earlier procedure of scraping without any changes\n",
        "* To save your checkpoints : `Object.save(pre=prefix_target_directory)`, they will be saved named as `UNIX_TIMESTAMP.pkl`, so that their dates are never lost, and they are kept in their natural order\n",
        "* To dump your data into your csv and stats to pkl files : `Object.dump(pre=prefix_target_directory)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7NgSddaLO4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize your crawler\n",
        "red=Crawler(size=500, difference=1, sleep=3)\n",
        "\n",
        "#Uncomment to scrape from beginning\n",
        "#for i in tqdm_notebook(range(3)):\n",
        "#  red.query()\n",
        "\n",
        "# load checkpoints\n",
        "red.load('checkpoints/1523347657.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1TkVu5kbvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "0aeaac00cbc5411daab1ac54ca6342c6",
            "d4e56c47a8694a668d32acf470b60c63",
            "ca2411485ab7438cb1caed40c611fdbf",
            "6431ff0f780b4f96833bf0706bfa82d1",
            "86ec3a0e3a2241d6b54552abb4591c4f",
            "5000b432518545768c9a5cb3e6e1586e",
            "47b370f03d59440b976db267ef608382",
            "148d90aa0cf34511a8e1bc373ba77c93"
          ]
        },
        "outputId": "e9844234-5760-4ee9-c0e1-b7e0f8b50e7d"
      },
      "source": [
        "#continue scraping with more days of scrape, the function remains the same just query to scrape a day of posts\n",
        "for i in tqdm_notebook(range(3)):\n",
        "  red.query()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0aeaac00cbc5411daab1ac54ca6342c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60b9tBMAjbBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red.dump(pre=\"data/\") #dump your data and stats\n",
        "red.save(\"checkpoints/\") #save your checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}